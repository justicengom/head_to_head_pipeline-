from pathlib import Path
import os
import math
from typing import Dict, Set, Union

from snakemake.utils import min_version

min_version("5.10.0")

GB = 1_024
PathLike = Union[str, Path, os.PathLike]

# ======================================================
# Config files
# ======================================================
configfile: "config.yaml"
containers: Dict[str, PathLike] = config["containers"]
envs: Dict[str, PathLike] = config["envs"]
assemblies: Dict[str, PathLike] = config["assemblies"]
fast5: Dict = config["fast5"]
fast5["outdir"] = Path(fast5["outdir"])
fast5["indir"] = Path(fast5["indir"])
samples: Set[str] = {*assemblies.keys()}
model_training_params = config["model_training_params"]

# ======================================================
# Global functions and variables
# ======================================================
outdir = Path(config["outdir"])
training_dir = outdir / "training"
evaluation_dir = outdir / "evaluation"
data_dir = Path(config["data_dir"])
mada_dir = data_dir / "madagascar"
ont_dir = mada_dir / "nanopore"
training_fast5_dir = fast5["outdir"] / "training"
evaluation_fast5_dir = fast5["outdir"] / "evaluation"

output_files = set()

for sample in samples:
    output_files.add(outdir / f"{sample}" / f"{sample}.read_references.fasta")
output_files.add(evaluation_fast5_dir / "filename_mapping.txt")
output_files.add(training_dir / "model" / "model.done")

# ======================================================
# Rules
# ======================================================
localrules: all, extract_read_ids, subset_read_ids, convert_model_json_to_checkpoint

rule all:
    input: output_files

# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#bam-of-mapped-basecalls
rule map_basecalls:
    input:
        reads=ont_dir / "{sample}" / "{sample}.nanopore.fastq.gz",
        assembly=lambda wildcards: assemblies[wildcards.sample],
    output:
        bam=outdir / "{sample}" / "mapping" / "{sample}.basecalls_mapped_to_asm.bam"
    shadow: "shallow"
    threads: 8
    resources:
        mem_mb=lambda wildcards, attempt: (4 * GB) * attempt
    singularity: containers["conda"]
    conda: envs["aln_tools"]
    params:
        preset="map-ont",
        minimap_extras=" ".join([
            "--secondary=no",  # don't output secondary alignments
            "-a",  # output SAM
            "-L",  # Write CIGAR with >65535 operators at the CG tag
            "--sam-hit-only",  # don't output unmapped reads
        ]),
        samtools_extras=" ".join([
            "-b",  # output BAM
        ]),
    shell:
        """
        minimap2 -x {params.preset} \
            {params.minimap_extras} \
            -t {threads} \
            {input.assembly} \
            {input.reads} | \
        samtools view {params.samtools_extras} -T {input.assembly} -o {output.bam}
        """

# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#extract-per-read-references
rule extract_per_read_references:
    input:
        assembly=lambda wildcards: assemblies[wildcards.sample],
        bam=rules.map_basecalls.output.bam
    output:
        read_references=outdir / "{sample}" / "{sample}.read_references.fasta"
    params:
        min_covg=config["min_covg"],
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: int(0.5 * GB) * attempt
    singularity: containers["taiyaki"]
    shell:
        """
        get_refs_from_sam.py --min_coverage {params.min_covg} \
            {input.assembly} \
            {input.bam} > {output.read_references}
        """

rule aggregate_read_references:
    input:
        read_references=[
            outdir / f"{sample}" / f"{sample}.read_references.fasta"
            for sample in samples
        ],
    output:
        read_references=outdir / "read_references.fasta",
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: int(0.5 * GB) * attempt
    shell:
        """
        awk 1 {input.read_references} > {output.read_references}
        """


rule extract_read_ids:
    input:
        read_references=rules.aggregate_read_references.output.read_references,
    output:
        read_ids=outdir / "read_ids.txt",
    threads: 1
    resources:
        mem_mb=int(0.3 * GB)
    params:
        pattern="'^>(?P<id>[\w-]+)\s*$'",
        replace_with="'$id'",
        extras="-uuu --no-line-number",  # disable smart filtering with -uuu
    shell:
        """
        rg --only-matching {params.pattern} --replace {params.replace_with} {params.extras} {input.read_references} > {output.read_ids}
        """


rule subset_read_ids:
    input:
        read_ids=rules.extract_read_ids.output.read_ids
    output:
        training_read_ids=outdir / "read_ids.training.txt",
        evaluation_read_ids=outdir / "read_ids.evaluation.txt"
    threads: 1
    resources:
        mem_mb=int(0.3 * GB)
    params:
        seed = config["seed"],
        training_size = config["training_size"]
    run:
        import random
        from pathlib import Path

        read_ids = []
        with open(input.read_ids) as fh:
            read_ids = [read_id for read_id in map(str.rstrip, fh) if read_id]

        random.seed(params.seed)
        k = int(len(read_ids) * params.training_size)
        print("Using {} reads for training, leaving {} for evaluation.".format(k, len(read_ids) - k))
        training_ids = set(random.sample(read_ids, k=k))
        # write training ids to file
        Path(output.training_read_ids).write_text("\n".join(training_ids))
        # write all other ids (evaluation) to file
        Path(output.evaluation_read_ids).write_text(
            "\n".join(i for i in read_ids if i not in training_ids)
        )



rule extract_fast5s_for_training:
    input:
        read_ids=rules.subset_read_ids.output.training_read_ids,
    output:
        filename_mapping=training_fast5_dir / "filename_mapping.txt",
    threads: 32
    resources:
        mem_mb=lambda wildcards, attempt: attempt * (16 * GB)
    singularity: containers["fast5"]
    params:
        indir=fast5["indir"],
        save_path=lambda wildcards, output: Path(output.filename_mapping).parent,
        batch_size=fast5["batch_size"],
        prefix="training",
        extras="--recursive"
    shell:
        """
        fast5_subset --input {params.indir} \
            --save_path {params.save_path} \
            --read_id_list {input.read_ids} \
            --batch_size {params.batch_size} \
            --filename_base {params.prefix} \
            --threads {threads} \
            {params.extras}

        tmpfile=$(mktemp)
        # add header row
        echo -e 'read_id\tfilename' | \
        awk 1 - {output.filename_mapping} > ${{tmpfile}} && mv ${{tmpfile}} {output.filename_mapping}
        """


rule extract_fast5s_for_evaluation:
    input:
        read_ids=rules.subset_read_ids.output.evaluation_read_ids,
    output:
        filename_mapping=evaluation_fast5_dir / "filename_mapping.txt",
    threads: 32
    resources:
        mem_mb=lambda wildcards, attempt: attempt * (16 * GB)
    singularity: containers["fast5"]
    params:
        indir=fast5["indir"],
        save_path=lambda wildcards, output: Path(output.filename_mapping).parent,
        batch_size=fast5["batch_size"],
        prefix="evaluation",
        extras="--recursive"
    shell:
        """
        fast5_subset --input {params.indir} \
            --save_path {params.save_path} \
            --read_id_list {input.read_ids} \
            --batch_size {params.batch_size} \
            --filename_base {params.prefix} \
            --threads {threads} \
            {params.extras}

        tmpfile=$(mktemp)
        # add header row
        echo -e 'read_id\tfilename' | \
        awk 1 - {output.filename_mapping} > ${{tmpfile}} && mv ${{tmpfile}} {output.filename_mapping}
        """


# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#create-per-read-scaling-parameters
rule create_per_read_scaling_params:
    input:
        file_summary=rules.extract_fast5s_for_training.output.filename_mapping,
    output:
        read_params=training_dir / "read_params.tsv",
    threads: 32
    resources:
        mem_mb=lambda wildcards, attempt: (6 * GB) * attempt
    singularity: containers["taiyaki"]
    params:
        reads_dir=lambda wildcards, input: Path(input.file_summary).parent,
        trim_start=fast5["trim"]["start"],
        trim_end=fast5["trim"]["end"],
        extras="--recursive",
    shell:
        """
        generate_per_read_params.py --jobs {threads} \
            --output {output.read_params} \
            --trim {params.trim_start} {params.trim_end} \
            --input_strand_list {input.file_summary} \
            {params.extras} \
            {params.reads_dir}
        """


rule convert_model_json_to_checkpoint:
    input:
        json=config["model"],
    output:
        checkpoint=Path(config["model"]).with_suffix(".checkpoint"),
    threads: 1
    resources:
        mem_mb=int(0.2 * GB)
    singularity: containers["taiyaki"]
    shell:
        """
        json_to_checkpoint.py --output {output.checkpoint} {input.json}
        """


# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#create-mapped-read-file
rule create_mapped_read_file:
    input:
        file_summary=rules.extract_fast5s_for_training.output.filename_mapping,
        read_params=rules.create_per_read_scaling_params.output.read_params,
        references=rules.aggregate_read_references.output.read_references,
        checkpoint=rules.convert_model_json_to_checkpoint.output.checkpoint,
    output:
        hdf5=training_dir / "mapped_reads.hdf5",
    threads: 32
    resources:
        mem_mb=lambda wildcards, attempt: attempt * int(64 * GB),
    params:
        extras="--recursive",
        reads_dir=lambda wildcards, input: Path(input.file_summary).parent,
    singularity: containers["taiyaki"]
    shell:
        """
        prepare_mapped_reads.py --jobs {threads} \
            --input_strand_list {input.file_summary} \
            {params.reads_dir} \
            {input.read_params} \
            {output.hdf5} \
            {input.checkpoint} \
            {input.references}
        """


def calculate_starting_learning_rate() -> int:
    scale_by = math.sqrt(model_training_params["num_gpus"])
    base_lrate = model_training_params["base_learning_rate"]
    return base_lrate * scale_by

# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#train-a-model
# https://github.com/nanoporetech/taiyaki#guppy-compatibility
rule train_model:
    input:
        mapped_reads=rules.create_mapped_read_file.output.hdf5
    output:
        checkpoint=training_dir / "model" / "model.done"
    threads: 2
    resources:
        mem_mb=int(60 * GB)
    params:
        model=model_training_params["model"],
        chunk_len_min = model_training_params["chunk_len_min"],
        chunk_len_max = model_training_params["chunk_len_max"],
        size = model_training_params["size"],
        stride = model_training_params["stride"],
        winlen = model_training_params["winlen"],
        outdir=lambda wildcards, output: Path(output.checkpoint).parent,
        num_gpus=model_training_params["num_gpus"],
        starting_learning_rate=calculate_starting_learning_rate(),
        extras="--overwrite"  # because snakemake creates this directory first taiyaki thinks a model already exists
    singularity: containers["taiyaki"]
    shell:
        """
        OPENBLAS_NUM_THREADS=1
        export OPENBLAS_NUM_THREADS
        OMP_NUM_THREADS={threads}
        export OMP_NUM_THREADS

        # have to use absolute path in container to train_flipflop script
        python3 -m torch.distributed.launch --nproc_per_node {params.num_gpus} \
        /taiyaki/bin/train_flipflop.py {params.extras} \
            --size {params.size} \
            --stride {params.stride} \
            --winlen {params.winlen} \
            --lr_max {params.starting_learning_rate} \
            --chunk_len_min {params.chunk_len_min} \
            --chunk_len_max {params.chunk_len_max} \
            --outdir {params.outdir} \
            {params.model} {input.mapped_reads}
        touch {output.checkpoint}
        """