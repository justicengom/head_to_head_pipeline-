from pathlib import Path
import os
import math
from typing import Dict, Set, Union, List
from itertools import product

from snakemake.utils import min_version

min_version("5.14.0")

GB = 1_024
PathLike = Union[str, Path, os.PathLike]


# ======================================================
# Config files
# ======================================================
configfile: "config.yaml"


containers: Dict[str, PathLike] = config["containers"]
envs: Dict[str, PathLike] = config["envs"]
scripts: Dict[str, PathLike] = config["scripts"]
assemblies: Dict[str, PathLike] = config["assemblies"]
report_notebook = Path(config["report_notebook"])
report_dir = report_notebook.parent
fast5: Dict = config["fast5"]
fast5["outdir"] = Path(fast5["outdir"])
fast5["indir"] = Path(fast5["indir"])
samples: Set[str] = {*assemblies.keys()}
models = {"guppy", "tubby"}
model_training_params = config["model_training_params"]
rule_log_dir = Path("logs/stderr").resolve()

# ======================================================
# Global functions and variables
# ======================================================
outdir = Path(config["outdir"])
training_dir = outdir / "training"
evaluation_dir = outdir / "evaluation"
data_dir = Path(config["data_dir"])
mada_dir = data_dir / "madagascar"
ont_dir = mada_dir / "nanopore"
training_fast5_dir = fast5["outdir"] / "training"
evaluation_fast5_dir = fast5["outdir"] / "evaluation"

output_files = set()

for sample, model in product(samples, models):
    output_files.add(
        evaluation_dir / f"{model}" / f"{sample}.{model}.substitution_counts.tsv"
    )
output_files.add(report_dir / "read-identity.png")
output_files.add(training_dir / "model/model.json.gz")


# ======================================================
# Rules
# ======================================================
localrules:
    all,
    extract_read_ids,
    subset_read_ids,
    convert_model_json_to_checkpoint,
    compress_model,


rule all:
    input:
        output_files,


# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#bam-of-mapped-basecalls
rule map_basecalls:
    input:
        reads=ont_dir / "{sample}" / "{sample}.nanopore.fastq.gz",
        assembly=lambda wildcards: assemblies[wildcards.sample],
    output:
        bam=outdir / "{sample}" / "mapping" / "{sample}.basecalls_mapped_to_asm.bam",
    shadow:
        "shallow"
    threads: 8
    resources:
        mem_mb=lambda wildcards, attempt: (4 * GB) * attempt,
    singularity:
        containers["conda"]
    conda:
        envs["aln_tools"]
    params:
        preset="map-ont",
        minimap_extras=" ".join(
            [
                "--secondary=no",  # don't output secondary alignments
                "-a",  # output SAM
                "-L",  # Write CIGAR with >65535 operators at the CG tag
                "--sam-hit-only",  # don't output unmapped reads
            ]
        ),
        samtools_extras=" ".join(["-b",]), # output BAM,
    shell:
        """
        minimap2 -x {params.preset} \
            {params.minimap_extras} \
            -t {threads} \
            {input.assembly} \
            {input.reads} | \
        samtools view {params.samtools_extras} -T {input.assembly} -o {output.bam}
        """


# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#extract-per-read-references
rule extract_per_read_references:
    input:
        assembly=lambda wildcards: assemblies[wildcards.sample],
        bam=rules.map_basecalls.output.bam,
    output:
        read_references=outdir / "{sample}" / "{sample}.read_references.fasta",
    params:
        min_covg=config["min_covg"],
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: int(0.5 * GB) * attempt,
    singularity:
        containers["taiyaki"]
    shell:
        """
        get_refs_from_sam.py --min_coverage {params.min_covg} \
            {input.assembly} \
            {input.bam} > {output.read_references}
        """


rule aggregate_read_references:
    input:
        read_references=[
            outdir / f"{sample}" / f"{sample}.read_references.fasta"
            for sample in samples
        ],
    output:
        read_references=outdir / "read_references.fasta",
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: int(0.5 * GB) * attempt,
    shell:
        """
        awk 1 {input.read_references} > {output.read_references}
        """


rule extract_read_ids:
    input:
        read_references=rules.aggregate_read_references.output.read_references,
    output:
        read_ids=outdir / "read_ids.txt",
    threads: 1
    resources:
        mem_mb=int(0.3 * GB),
    params:
        pattern="'^>(?P<id>[\w-]+)\s*$'",
        replace_with="'$id'",
        extras="-uuu --no-line-number", # disable smart filtering with -uuu
    shell:
        """
        rg --only-matching {params.pattern} --replace {params.replace_with} {params.extras} {input.read_references} > {output.read_ids}
        """


rule subset_read_ids:
    input:
        read_ids=rules.extract_read_ids.output.read_ids,
    output:
        training_read_ids=outdir / "read_ids.training.txt",
        evaluation_read_ids=outdir / "read_ids.evaluation.txt",
    threads: 1
    resources:
        mem_mb=int(0.3 * GB),
    params:
        seed=config["seed"],
        training_size=config["training_size"],
    run:
        import random
        from pathlib import Path

        read_ids = []
        with open(input.read_ids) as fh:
            read_ids = [read_id for read_id in map(str.rstrip, fh) if read_id]

        random.seed(params.seed)
        k = int(len(read_ids) * params.training_size)
        print(
            "Using {} reads for training, leaving {} for evaluation.".format(
                k, len(read_ids) - k
            )
        )
        training_ids = set(random.sample(read_ids, k=k))
        # write training ids to file
        Path(output.training_read_ids).write_text("\n".join(training_ids))
        # write all other ids (evaluation) to file
        Path(output.evaluation_read_ids).write_text(
            "\n".join(i for i in read_ids if i not in training_ids)
        )


rule extract_fast5s_for_training:
    input:
        read_ids=rules.subset_read_ids.output.training_read_ids,
    output:
        filename_mapping=training_fast5_dir / "filename_mapping.txt",
    threads: 32
    resources:
        mem_mb=lambda wildcards, attempt: attempt * (16 * GB),
    singularity:
        containers["fast5"]
    params:
        indir=fast5["indir"],
        save_path=lambda wildcards, output: Path(output.filename_mapping).parent,
        batch_size=fast5["batch_size"],
        prefix="training",
        extras="--recursive",
    shell:
        """
        fast5_subset --input {params.indir} \
            --save_path {params.save_path} \
            --read_id_list {input.read_ids} \
            --batch_size {params.batch_size} \
            --filename_base {params.prefix} \
            --threads {threads} \
            {params.extras}

        tmpfile=$(mktemp)
        # add header row
        echo -e 'read_id\tfilename' | \
        awk 1 - {output.filename_mapping} > ${{tmpfile}} && mv ${{tmpfile}} {output.filename_mapping}
        """


rule extract_fast5s_for_evaluation:
    input:
        read_ids=rules.subset_read_ids.output.evaluation_read_ids,
    output:
        filename_mapping=evaluation_fast5_dir / "filename_mapping.txt",
    threads: 32
    resources:
        mem_mb=lambda wildcards, attempt: attempt * (16 * GB),
    singularity:
        containers["fast5"]
    params:
        indir=fast5["indir"],
        save_path=lambda wildcards, output: Path(output.filename_mapping).parent,
        batch_size=fast5["batch_size"],
        prefix="evaluation",
        extras="--recursive",
    shell:
        """
        fast5_subset --input {params.indir} \
            --save_path {params.save_path} \
            --read_id_list {input.read_ids} \
            --batch_size {params.batch_size} \
            --filename_base {params.prefix} \
            --threads {threads} \
            {params.extras}

        tmpfile=$(mktemp)
        # add header row
        echo -e 'read_id\tfilename' | \
        awk 1 - {output.filename_mapping} > ${{tmpfile}} && mv ${{tmpfile}} {output.filename_mapping}
        """


# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#create-per-read-scaling-parameters
rule create_per_read_scaling_params:
    input:
        file_summary=rules.extract_fast5s_for_training.output.filename_mapping,
    output:
        read_params=training_dir / "read_params.tsv",
    threads: 32
    resources:
        mem_mb=lambda wildcards, attempt: (6 * GB) * attempt,
    singularity:
        containers["taiyaki"]
    params:
        reads_dir=lambda wildcards, input: Path(input.file_summary).parent,
        trim_start=fast5["trim"]["start"],
        trim_end=fast5["trim"]["end"],
        extras="--recursive",
    shell:
        """
        generate_per_read_params.py --jobs {threads} \
            --output {output.read_params} \
            --trim {params.trim_start} {params.trim_end} \
            --input_strand_list {input.file_summary} \
            {params.extras} \
            {params.reads_dir}
        """


rule convert_model_json_to_checkpoint:
    input:
        json=config["model"],
    output:
        checkpoint=Path(config["model"]).with_suffix(".checkpoint"),
    threads: 1
    resources:
        mem_mb=int(0.2 * GB),
    singularity:
        containers["taiyaki"]
    shell:
        """
        json_to_checkpoint.py --output {output.checkpoint} {input.json}
        """


# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#create-mapped-read-file
rule create_mapped_read_file:
    input:
        file_summary=rules.extract_fast5s_for_training.output.filename_mapping,
        read_params=rules.create_per_read_scaling_params.output.read_params,
        references=rules.aggregate_read_references.output.read_references,
        checkpoint=rules.convert_model_json_to_checkpoint.output.checkpoint,
    output:
        hdf5=training_dir / "mapped_reads.hdf5",
    threads: 32
    resources:
        mem_mb=lambda wildcards, attempt: attempt * int(64 * GB),
    params:
        extras="--recursive",
        reads_dir=lambda wildcards, input: Path(input.file_summary).parent,
    singularity:
        containers["taiyaki"]
    shell:
        """
        prepare_mapped_reads.py --jobs {threads} \
            --input_strand_list {input.file_summary} \
            {params.reads_dir} \
            {input.read_params} \
            {output.hdf5} \
            {input.checkpoint} \
            {input.references}
        """


def calculate_starting_learning_rate() -> int:
    scale_by = math.sqrt(model_training_params["num_gpus"])
    base_lrate = model_training_params["base_learning_rate"]
    return base_lrate * scale_by


# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#train-a-model
# https://github.com/nanoporetech/taiyaki#guppy-compatibility
rule train_model:
    input:
        mapped_reads=rules.create_mapped_read_file.output.hdf5,
    output:
        checkpoint=training_dir / "model" / "model_final.checkpoint",
    threads: 2
    resources:
        mem_mb=int(60 * GB),
    params:
        model=model_training_params["model"],
        chunk_len_min=model_training_params["chunk_len_min"],
        chunk_len_max=model_training_params["chunk_len_max"],
        size=model_training_params["size"],
        stride=model_training_params["stride"],
        winlen=model_training_params["winlen"],
        outdir=lambda wildcards, output: Path(output.checkpoint).parent,
        num_gpus=model_training_params["num_gpus"],
        starting_learning_rate=calculate_starting_learning_rate(),
        extras="--overwrite", # because snakemake creates this directory first taiyaki thinks a model already exists
    singularity:
        containers["taiyaki"]
    shell:
        """
        OPENBLAS_NUM_THREADS=1
        export OPENBLAS_NUM_THREADS
        OMP_NUM_THREADS={threads}
        export OMP_NUM_THREADS

        # have to use absolute path in container to train_flipflop script
        python3 -m torch.distributed.launch --nproc_per_node {params.num_gpus} \
        /taiyaki/bin/train_flipflop.py {params.extras} \
            --size {params.size} \
            --stride {params.stride} \
            --winlen {params.winlen} \
            --lr_max {params.starting_learning_rate} \
            --chunk_len_min {params.chunk_len_min} \
            --chunk_len_max {params.chunk_len_max} \
            --outdir {params.outdir} \
            {params.model} {input.mapped_reads}
        """


# https://github.com/nanoporetech/taiyaki/blob/master/docs/walkthrough.rst#export-to-guppy
rule export_to_guppy:
    input:
        checkpoint=rules.train_model.output.checkpoint,
    output:
        model=training_dir / "model" / "model.json",
    singularity:
        containers["taiyaki"]
    shell:
        """
        dump_json.py {input.checkpoint} > {output.model}
        """


rule compress_model:
    input:
        model=rules.export_to_guppy.output.model,
    output:
        model=training_dir / "model/model.json.gz",
    log:
        rule_log_dir / "compress_model.log",
    params:
        options="-9 -c",
    shell:
        "gzip {params.options} {input.model} > {output.model} 2> {log}"


rule basecall_evaluation_data:
    input:
        mapping=rules.extract_fast5s_for_evaluation.output.filename_mapping,
        model=rules.export_to_guppy.output.model,
    output:
        summary=evaluation_dir / "basecall" / "sequencing_summary.txt",
        fastq=evaluation_dir / "basecall" / "tubby.fastq.gz",
    threads: 2
    resources:
        mem_mb=int(8 * GB),
    singularity:
        containers["guppy-gpu"]
    params:
        num_callers=8,
        fast5_dir=lambda wildcards, input: Path(input.mapping).parent,
        save_path=lambda wildcards, output: Path(output.summary).parent,
        fastq_outdir=lambda wildcards, output: Path(output.summary).parent / "fastqs",
        device="cuda:all:100%",
        config=config["model_config"],
        extras=" ".join(["--recursive", "--compress_fastq"]),
    shell:
        """
        guppy_basecaller \
                    {params.extras} \
                    --input_path {params.fast5_dir} \
            --save_path {params.save_path} \
            --config {params.config} \
            --device {params.device} \
            --num_callers {params.num_callers} \
            --model_file {input.model}

        mkdir -p {params.fastq_outdir} && mv {params.save_path}/*.fastq.gz {params.fastq_outdir}
        cat {params.fastq_outdir}/*.fastq.gz > {output.fastq}
        """


rule map_id_to_sample_eval:
    """Create a file mapping read IDs to the sample that read originally came from"""
    input:
        read_ids=rules.subset_read_ids.output.evaluation_read_ids,
        fastqs=expand(
            f"{ont_dir}/{{sample}}/{{sample}}.nanopore.fastq.gz", sample=samples
        ),
    output:
        id_map=evaluation_dir / "id_to_sample_map.csv",
    threads: 4
    resources:
        mem_mb=lambda wildcards, attempt: int(GB * 16) * attempt,
    params:
        pattern="'.*\/(?<sample>mada_[0-9\-]+)\/.*:(?<id>[\w\d\-]+)$'",
        output_format="'$id,$sample'",
        extras="--no-heading --no-line-number --only-matching --search-zip",
    shell:
        """
        echo 'id,sample' > {output.id_map}
        rg {params.extras} --threads {threads} \
            --file {input.read_ids} \
            {input.fastqs} | \
        rg --pcre2 {params.pattern} \
            --replace {params.output_format} >> {output.id_map}
        """


def infer_fastq_for_model(wildcards) -> List[str]:
    model = wildcards.model
    sample = wildcards.sample

    if model == "guppy":
        return [str(ont_dir / f"{sample}" / f"{sample}.nanopore.fastq.gz")]
    elif model == "tubby":
        return [str(evaluation_dir / "basecall" / "tubby.fastq.gz")]
    else:
        raise ValueError(f"Got unexpected model. Wildcards: {wildcards}")


rule create_eval_fastqs:
    input:
        fastqs=infer_fastq_for_model,
        id_map=rules.map_id_to_sample_eval.output.id_map,
    output:
        fastq=evaluation_dir / "{model}" / "{sample}.{model}.fq.gz",
    resources:
        mem_mb=int(4 * GB),
    run:
        from pyfastaq import sequences
        import gzip

        read_ids_for_sample = set()
        with open(input.id_map) as csvfile:
            for row in map(str.rstrip, csvfile):
                read_id, sample = row.split(",")
                if sample == wildcards.sample:
                    read_ids_for_sample.add(read_id)

        out_fq = gzip.open(output.fastq, "wb")
        for fastq in input.fastqs:
            in_fq = sequences.file_reader(fastq)
            for record in in_fq:
                rid = record.id.split()[0]
                if rid in read_ids_for_sample:
                    content = str(record) + "\n"
                    out_fq.write(content.encode("utf-8"))
            in_fq.close()
        out_fq.close()


rule map_basecalls_to_reference:
    input:
        query=rules.create_eval_fastqs.output.fastq,
        target=lambda wildcards: assemblies[wildcards.sample],
    output:
        paf=evaluation_dir / "{model}" / "{sample}.{model}.paf",
    threads: 8
    resources:
        mem_mb=int(10 * GB),
    singularity:
        containers["conda"]
    conda:
        envs["aln_tools"]
    params:
        preset="map-ont",
        extras=" ".join(
            [
                "--secondary=no",  # don't output secondary alignments
                "-2",  # use two threads for IO
                "-L",  # Write CIGAR with >65535 operators at the CG tag
                "-c",  # generate CIGAR
            ]
        ),
    shell:
        """
        minimap2 -x {params.preset} \
            {params.extras} \
            -t {threads} \
            -o {output.paf} \
            {input.target} \
            {input.query}
        """


rule calculate_read_blast_identity:
    """Aggregate for each model. For each read from all samples, calculate the BLAST
    identity from the PAF files. Look into whether I need to add some kind of coverage
    threshold on this to make sure we aren't assessing small alignments."""
    input:
        paf=rules.map_basecalls_to_reference.output.paf,
    output:
        csv=evaluation_dir / "{model}" / "{sample}.{model}.read_identity.csv",
    threads: 1
    resources:
        mem_mb=int(0.5 * GB),
    params:
        script=scripts["read_identity"],
        delim=",",
        min_cov=0.5, # only assess reads where more than half the read align
        extras="--primary-only",
    singularity:
        containers["conda"]
    conda:
        envs["paf"]
    shell:
        """
        python {params.script} -i {input.paf} \
            -o {output.csv} \
            --delim {params.delim} \
            --min-cov {params.min_cov} \
            {params.extras}
        """


rule rebaler:
    input:
        reads=rules.create_eval_fastqs.output.fastq,
        reference=lambda wildcards: assemblies[wildcards.sample],
    output:
        assembly=(
            evaluation_dir / "{model}" / "rebaler" / "{sample}.{model}.rebaler.fa"
        ),
    shadow:
        "shallow"
    threads: 8
    resources:
        mem_mb=lambda wildcards, attempt: int(8 * GB) * attempt,
    singularity:
        containers["rebaler"]
    shell:
        """
        rebaler --threads {threads} {input.reference} {input.reads} > {output.assembly}
        """


rule chop_assemblies:
    input:
        assembly=rules.rebaler.output.assembly,
    output:
        chunks=evaluation_dir / "{model}" / "{sample}.{model}.rebaler_asm_chunks.fa",
    threads: 1
    resources:
        mem_mb=int(0.5 * GB),
    params:
        script=scripts["chop_assembly"],
        min_tail_len=1_000,
        chunk_size=10_000,
    singularity:
        containers["conda"]
    conda:
        envs["chop"]
    shell:
        """
        python {params.script} -i {input.assembly} \
            -o {output.chunks} \
            --chunk-size {params.chunk_size} \
            --min-tail-size {params.min_tail_len}
        """


#
rule map_asm_pieces_to_reference:
    """https://github.com/rrwick/Basecalling-comparison/blob/40c4802b3e78d262f0d6f07851e4e648bff39670/analysis_scripts/analysis.sh#L139"""
    input:
        query=rules.chop_assemblies.output.chunks,
        target=lambda wildcards: assemblies[wildcards.sample],
    output:
        paf=evaluation_dir / "{model}" / "{sample}.{model}.rebaler_asm_chunks.paf",
    threads: 8
    resources:
        mem_mb=int(10 * GB),
    singularity:
        containers["conda"]
    conda:
        envs["aln_tools"]
    params:
        preset="asm5",
        extras=" ".join(
            [
                "--secondary=no",  # don't output secondary alignments
                "-2",  # use two threads for IO
                "-L",  # Write CIGAR with >65535 operators at the CG tag
                "-c",  # generate CIGAR
            ]
        ),
    shell:
        """
        minimap2 -x {params.preset} \
            {params.extras} \
            -t {threads} \
            -o {output.paf} \
            {input.target} \
            {input.query}
        """


rule calculate_consensus_accuracy:
    """https://github.com/rrwick/Basecalling-comparison/blob/40c4802b3e78d262f0d6f07851e4e648bff39670/analysis_scripts/analysis.sh#L140"""
    """Aggregate for each model. For each read from all samples, calculate the BLAST
    identity from the PAF files. Look into whether I need to add some kind of coverage
    threshold on this to make sure we aren't assessing small alignments."""
    input:
        paf=rules.map_asm_pieces_to_reference.output.paf,
    output:
        csv=evaluation_dir / "{model}" / "{sample}.{model}.consensus_identity.csv",
    threads: 1
    resources:
        mem_mb=int(0.5 * GB),
    params:
        script=scripts["read_identity"],
        delim=",",
        min_cov=0.5, # only assess when over half the chunk aligns
        extras="--primary-only",
    singularity:
        containers["conda"]
    conda:
        envs["paf"]
    shell:
        """
        python {params.script} -i {input.paf} \
            -o {output.csv} \
            --delim {params.delim} \
            --min-cov {params.min_cov} \
            {params.extras}
        """


#
rule calculate_consensus_errors:
    """https://github.com/rrwick/Basecalling-comparison/blob/40c4802b3e78d262f0d6f07851e4e648bff39670/analysis_scripts/analysis.sh#L142-L151"""
    input:
        assembly=lambda wildcards: assemblies[wildcards.sample],
        rebaler_asm=rules.rebaler.output.assembly,
    output:
        err_details=(
            evaluation_dir / "{model}" / "{sample}.{model}.assembly_error_details.tsv"
        ),
        sub_counts=(
            evaluation_dir / "{model}" / "{sample}.{model}.substitution_counts.tsv"
        ),
    threads: 1
    resources:
        mem_mb=int(4 * GB),
    singularity:
        containers["conda"]
    conda:
        envs["mummer"]
    shadow:
        "shallow"
    params:
        error_summary_script=scripts["error_summary"],
        prefix=lambda wildcards: f"{wildcards.sample}.{wildcards.model}",
        context=5,
        delta_extras=" ".join(
            [
                "-r",  # Maps each position of each reference to its best hit in the query, allowing for query overlaps
                "-q",  # Maps each position of each query to its best hit in the reference, allowing for reference overlaps
            ]
        ),
        snps_extras=" ".join(
            [
                "-C",  # Do not report SNPs from alignments with an ambiguous mapping
                "-l",  # Include sequence length information in the output
                "-r",  # Sort output lines by reference IDs and SNP positions
                "-T",  # Switch to tab-delimited format
                "-H",  # Do not print the output header
            ]
        ),
    shell:
        """
        printf "assembly\tdcm\thomo_del\thomo_ins\tother_del\tother_ins\tsub\n" > {output.err_details}
        printf "{params.prefix}\t" >> {output.err_details}
        nucmer --prefix={params.prefix} {input.assembly} {input.rebaler_asm}
        delta-filter {params.delta_extras} {params.prefix}.delta > {params.prefix}.filter
        show-snps {params.snps_extras} \
            -x{params.context} {params.prefix}.filter | \
          python3 {params.error_summary_script} >> {output.err_details}
        printf "{params.prefix}\tassembly\t" >> {output.sub_counts}
        show-snps {params.snps_extras} {params.prefix}.filter | \
          awk '$2 != "." && $3 != "."' | \
          wc -l >> {output.sub_counts}
        rm {params.prefix}.delta {params.prefix}.filter
        """


rule report:
    input:
        asm_error_tsvs=expand(
            f"{evaluation_dir}/{{model}}/{{sample}}.{{model}}.assembly_error_details.tsv",
            sample=samples,
            model=models,
        ),
        consensus_identity_csvs=expand(
            f"{evaluation_dir}/{{model}}/{{sample}}.{{model}}.consensus_identity.csv",
            sample=samples,
            model=models,
        ),
        read_identity_csvs=expand(
            f"{evaluation_dir}/{{model}}/{{sample}}.{{model}}.read_identity.csv",
            sample=samples,
            model=models,
        ),
    output:
        read_identity=report_dir / "read-identity.png",
        read_relative_len=report_dir / "read-relative-len.png",
        read_identity_vs_len=report_dir / "read-identity-vs-len.png",
        consensus_identity=report_dir / "consensus-identity.png",
        consensus_relative_len=report_dir / "consensus-relative-len.png",
        error_types=report_dir / "consensus-error-types.png",
        combined_plot=report_dir / "combined_identity_relative_len.png",
    threads: 1
    resources:
        mem_mb=int(2 * GB),
    singularity:
        containers["conda"]
    conda:
        envs["report"]
    params:
        style="ggplot",
        palette="Set1",
        figsize=(13, 8),
        dpi=300,
        subsample=100_000,
    log:
        notebook=report_dir / "processed-report.ipynb",
    notebook:
        str(report_notebook)
