data_dir:        "/hps/nobackup/research/zi/projects/tech_wars/data"
outdir:          "results"
model_config:    "dna_r9.4.1_450bps_hac_prom.cfg"  # the model to use when basecalling eval data
report_notebook: "report/report.py.ipynb"
min_covg:        0.8  # Ignore reads with alignments shorter than min_covg * read length
model:           "resources/pretrained_models/guppy-gpu.v3-4-5.template_r9.4.1_450bps_hac_prom.jsn"  # model used to basecall original data
seed:            88  # random seed to use for randomly subsample read ids into training set
training_size:   0.2  # percentage of reads to use for training - refer to https://github.com/nanoporetech/taiyaki/issues/73
model_training_params:  # hard-coding defaults as of 10/04/2020 https://github.com/nanoporetech/taiyaki#standard-model-parameters
  model:              "results/training/model/model_checkpoint_00006.checkpoint"
  chunk_len_min:      2000
  chunk_len_max:      4000
  size:               256
  stride:             2
  winlen:             19
  num_gpus:           2
  base_learning_rate: 0.002

fast5:
  indir:      "/hps/nobackup/research/zi/projects/tech_wars/data/madagascar/nanopore/raw_data/"  # directory containing all of the original fast5 files
  outdir:     "data/fast5s/"  # directory where fast5 reads used for training will be placed
  batch_size: 4000  # number of fast5 per multi-fast5
  trim:  # Number of samples to trim off start and end of fast5 training files
    start: 200
    end:   100

containers:
  conda:     "docker://continuumio/miniconda3:4.7.12"
  taiyaki:   "library://mbhall88/default/taiyaki:ce0cfbf"
  fast5:     "docker://quay.io/biocontainers/ont-fast5-api:3.0.1--py_0"
  guppy-gpu: "library://mbhall88/default/guppy-gpu:3.4.5"
  rebaler:   "docker://quay.io/biocontainers/rebaler:0.2.0--py_1"

envs:
  aln_tools:   "envs/aln_tools.yaml"
  paf:         "envs/paf.yaml"
  chop:        "envs/chop.yaml"
  mummer:      "envs/mummer.yaml"
  report:      "envs/report.yaml"

scripts:
  read_identity: "scripts/read_assessment.py"
  chop_assembly: "scripts/chop_assembly.py"
  error_summary: "scripts/error_summary.py"

assemblies:  # mada_1-2 is not being used as they are not of sufficient quality
  mada_101:  "/hps/nobackup/research/zi/projects/tech_wars/analysis/assembly/results/mada_101/flye/pacbio/polished_assembly.flye.pacbio.fasta"
  mada_102:  "/hps/nobackup/research/zi/projects/tech_wars/analysis/assembly/results/mada_102/spades/pacbio/polished_assembly.spades.pacbio.fasta"
  mada_104:  "/hps/nobackup/research/zi/projects/tech_wars/analysis/assembly/results/mada_104/flye/pacbio/polished_assembly.flye.pacbio.fasta"
  mada_116:  "/hps/nobackup/research/zi/projects/tech_wars/analysis/assembly/results/mada_116/unicycler/pacbio/decontam.assembly.unicycler.pacbio.fasta"
  mada_125:  "/hps/nobackup/research/zi/projects/tech_wars/analysis/assembly/results/mada_125/spades/pacbio/polished_assembly.spades.pacbio.fasta"
  mada_130:  "/hps/nobackup/research/zi/projects/tech_wars/analysis/assembly/results/mada_130/flye/pacbio/decontam.assembly.flye.pacbio.fasta"
  mada_132:  "/hps/nobackup/research/zi/projects/tech_wars/analysis/assembly/results/mada_132/spades/pacbio/polished_assembly.spades.pacbio.fasta"
  mada_1-44: "/hps/nobackup/research/zi/projects/tech_wars/analysis/assembly/results/mada_1-44/unicycler/pacbio/decontam.assembly.unicycler.pacbio.fasta"
