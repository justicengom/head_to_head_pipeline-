import os
from itertools import product
from pathlib import Path
from typing import Dict, Union

from snakemake.utils import min_version

min_version("5.14.0")

GB = 1_024
PathLike = Union[str, Path, os.PathLike]


# ======================================================
# Config files
# ======================================================
configfile: "config.yaml"


containers: Dict[str, PathLike] = config["containers"]
envs: Dict[str, PathLike] = config["envs"]
scripts: Dict[str, PathLike] = config["scripts"]
H37RV = config["h37rv"]
prg_dir = Path("prgs")
rare_lineages = set(config["rare_lineages"])
lineages = set(config["lineages"])
prg_names = config["prg_names"]
vcf_dir = Path("vcfs")
rule_log_dir = Path("logs/stderr")

# ======================================================
# Global functions and variables
# ======================================================
output_files = set()
for lineage, prg_name in product(lineages, prg_names.keys()):
    output_files.add(prg_dir / f"{prg_name}" / "local_prgs")


# ======================================================
# Rules
# ======================================================
rule all:
    input:
        output_files,


rule split_h37rv:
    input:
        genome=H37RV["genome"],
        features=H37RV["features"],
    output:
        info=prg_dir / "reference_loci" / "loci-info.csv",
    threads: 1
    resources:
        mem_mb=1 * GB,
    singularity:
        containers["conda"]
    conda:
        envs["gff_split"]
    params:
        script=scripts["gff_split"],
        outdir=lambda wildcards, output: Path(output.info).parent,
        types="gene",
        min_len=config["min_chunk_len"],
    shell:
        """
        python {params.script} --fasta {input.genome} \
            --gff {input.features} \
            --outdir {params.outdir} \
            --types {params.types} \
            --min-len {params.min_len} 
        """


rule assign_lineages:
    input:
        vcf=config["cryptic_vcf"],
        panel=config["lineage_panel"],
    output:
        assignments="resources/cryptic.lineages.csv",
    threads: 1
    resources:
        mem_mb=GB,
    singularity:
        containers["conda"]
    conda:
        envs["assign_lineages"]
    params:
        script=scripts["assign_lineages"],
        default_lineage=config["default_lineage"], # the name given to samples with no hits in the panel
        max_het=1,
        max_alt_lineages=1,
        ref_lineage_position=config["ref_lineage_position"],
        extras="--verbose",
    shell:
        """
        python {params.script} --input {input.vcf} \
            --panel {input.panel} \
            --output {output.assignments} \
            --default-lineage {params.default_lineage} \
            --max-het {params.max_het} \
            --ref-lineage-position {params.ref_lineage_position} \
            --max-alt-lineages {params.max_alt_lineages} {params.extras}
        """


rule create_samples_files:
    input:
        assignments=rules.assign_lineages.output.assignments,
    output:
        samples_file="resources/sample-list.L{lineage}.txt",
    threads: 1
    resources:
        mem_mb=int(0.2 * GB),
    run:
        samples = []
        with open(output.samples_file, "w") as outstream, open(input.assignments) as instream:
            for row in map(str.rstrip, instream):
                fields = row.split(",")
                sample = fields[0]
                lin = fields[1]
                if (
                    wildcards.lineage == "rare" and lin.lower() in rare_lineages
                ) or lin == wildcards.lineage:
                    print(sample, file=outstream)



rule split_into_lineage_vcfs:
    input:
        vcf=config["cryptic_vcf"],
        samples_file=rules.create_samples_files.output.samples_file,
    output:
        vcf=vcf_dir / "lineage" / "cryptic.L{lineage}.bcf.gz",
    threads: 4
    resources:
        mem_mb=GB,
    singularity:
        containers["bcftools"]
    params:
        output_type="b", # compressed BCF
        extras=" ".join(["--trim-alt-alleles", "--exclude-uncalled"]),
    shell:
        """
        bcftools view --threads {threads} \
            --samples-file {input.samples_file} \
            --output-type {params.output_type} \
            -o {output.vcf} \
            {params.extras} \
            {input.vcf}
        """


rule subsample_samples_files:
    input:
        samples_file=rules.create_samples_files.output.samples_file,
    output:
        samples_file="resources/sample-list.{prg_name}.L{lineage}.txt",
    threads: 1
    resources:
        mem_mb=int(0.3 * GB),
    params:
        seed=88,
    run:
        import random
        from pathlib import Path

        random.seed(params.seed)
        samples = list(filter(None, Path(input.samples_file).read_text().splitlines()))
        threshold = prg_names[wildcards.prg_name]
        k = min(int(threshold), len(samples))
        selections = random.sample(samples, k=k)
        content = "\n".join(selections)
        Path(output.samples_file).write_text(content)



rule subsample_vcfs:
    input:
        vcf=rules.split_into_lineage_vcfs.output.vcf,
        samples_file=rules.subsample_samples_files.output.samples_file,
    output:
        vcf=vcf_dir / "lineage" / "cryptic.{prg_name}.L{lineage}.bcf.gz",
    threads: 4
    resources:
        mem_mb=GB,
    singularity:
        containers["bcftools"]
    params:
        output_type="b", # compressed BCF
        extras=" ".join(["--trim-alt-alleles", "--exclude-uncalled"]),
    shell:
        """
        bcftools view --threads {threads} \
            --samples-file {input.samples_file} \
            --output-type {params.output_type} \
            -o {output.vcf} \
            {params.extras} \
            {input.vcf}
        """


rule index_vcfs:
    input:
        vcf=rules.subsample_vcfs.output.vcf,
    output:
        index=vcf_dir / "lineage" / "cryptic.{prg_name}.L{lineage}.bcf.gz.csi",
    threads: 4
    resources:
        mem_mb=GB,
    singularity:
        containers["bcftools"]
    params:
        extras=" ".join(["--csi", "--force"]),
    shell:
        """
        bcftools index --threads {threads} \
            {params.extras} \
            {input.vcf}
        """


rule merge_vcfs:
    input:
        cryptic_vcfs=expand(
            "vcfs/lineage/cryptic.{{prg_name}}.L{lineage}.bcf.gz", lineage=lineages,
        ),
        cryptic_indexes=expand(
            "vcfs/lineage/cryptic.{{prg_name}}.L{lineage}.bcf.gz.csi", lineage=lineages,
        ),
        comas_vcf=config["comas_vcf"],
    output:
        vcf=vcf_dir / "merged" / "{prg_name}.bcf.gz",
    threads: 4
    resources:
        mem_mb=4 * GB,
    singularity:
        containers["bcftools"]
    params:
        output_type="b", # compressed BCF
        extras="",
    shell:
        """
        bcftools merge --threads {threads} \
            --output-type {params.output_type} \
            -o {output.vcf} \
            {params.extras} \
            {input.cryptic_vcfs} {input.comas_vcf}
        """


rule filter_vcfs:
    input:
        vcf=rules.merge_vcfs.output.vcf,
        mask=H37RV["mask"],
    output:
        vcf=vcf_dir / "filtered" / "{prg_name}.filtered.vcf.gz",
        index=vcf_dir / "filtered" / "{prg_name}.filtered.vcf.gz.csi",
    threads: 4
    resources:
        mem_mb=2 * GB,
    singularity:
        containers["conda"]
    conda:
        envs["filter_vcf"]
    params:
        output_type="z", # compressed VCF
        filters=".,PASS",
        vcf_extras=" ".join(
            [
                "--trim-alt-alleles",
                "--exclude-uncalled",
                "--min-ac=1:nref",  # remove any positions without an ALT call
            ]
        ),
        bedtools_extras=" ".join(["-header", "-A",]), # remove entire feature if any overlap,
    shell:
        """
        bcftools view --threads {threads} \
            --apply-filters {params.filters} \
            --output-type v \
            {params.vcf_extras} \
            {input.vcf} | \
        bedtools subtract {params.bedtools_extras} -a - -b {input.mask} | \
        bcftools convert -O {params.output_type} -o {output.vcf} --threads {threads}

        bcftools index --threads {threads} --csi --force {output.vcf}
        """


rule associate_vcf_records_to_loci:
    input:
        vcf=rules.filter_vcfs.output.vcf,
        loci_info=rules.split_h37rv.output.info,
    output:
        vcf=vcf_dir / "final" / "{prg_name}.vcf.gz",
        index=vcf_dir / "final" / "{prg_name}.vcf.gz.csi",
    shadow:
        "shallow"
    threads: 4
    resources:
        mem_mb=2 * GB,
    singularity:
        containers["conda"]
    conda:
        envs["records_to_loci"]
    params:
        script=scripts["records_to_loci"],
        extras="--verbose",
    log:
        rule_log_dir / "associate_vcf_records_to_loci/{prg_name}.log",
    shell:
        """
        python {params.script} -i {input.vcf} \
            --loci-info {input.loci_info} \
            --output - \
            {params.extras} | bcftools sort -o {output.vcf} -O z --temp-dir . 2> {log}
        bcftools index --threads {threads} --force --csi {output.vcf} 2>> {log}
        """


rule apply_variants_to_loci:
    input:
        vcf=rules.associate_vcf_records_to_loci.output.vcf,
        loci_info=rules.split_h37rv.output.info,
    output:
        loci_dir=directory(prg_dir / "{prg_name}" / "applied_variants"),
    threads: 1
    resources:
        mem_mb=int(2 * GB),
    singularity:
        containers["conda"]
    conda:
        envs["apply_variants"]
    params:
        script=scripts["apply_variants"],
        loci_dir=lambda wildcards, input: str(Path(input.loci_info).parent),
        extras="--verbose",
    log:
        rule_log_dir / "apply_variants_to_loci/{prg_name}.log",
    shell:
        """
        python {params.script} {params.extras} \
            --vcf-path {input.vcf} \
            --outdir {output.loci_dir} \
            --loci-dir {params.loci_dir} \
            --loci-info {input.loci_info} 2> {log}
        """


# See https://github.com/sharkdp/fd#parallel-command-execution for clarification of the --exec syntax used for fd
rule multiple_sequence_alignment_of_loci:
    input:
        loci_dir=rules.apply_variants_to_loci.output.loci_dir,
    output:
        msa_dir=directory(prg_dir / "{prg_name}" / "multiple_sequence_alignments"),
    threads: 16
    resources:
        mem_mb=lambda wildcards, attempt: int(8 * GB) * attempt,
    singularity:
        containers["conda"]
    conda:
        envs["msa"]
    params:
        extras="--no-ignore --hidden",
        extension="fa",
        pattern=".",
        script=scripts["msa"],
    log:
        rule_log_dir / "multiple_sequence_alignment_of_loci/{prg_name}.log",
    shell:
        """
        mkdir -p {output.msa_dir}
        fd {params.extras} \
            --extension {params.extension} \
            --threads {threads} \
            --exec bash {params.script} '{{}}' {output.msa_dir}/'{{/}}' {log} \; \
            {params.pattern} {input.loci_dir} 2>> {log}
        """


rule build_local_prgs:
    input:
        msa_dir=rules.multiple_sequence_alignment_of_loci.output.msa_dir,
    output:
        local_prg_dir=directory(prg_dir / "{prg_name}" / "local_prgs"),
    shadow:
        "shallow"
    threads: 16
    resources:
        mem_mb=lambda wildcards, attempt: int(16 * GB) * attempt,
    singularity:
        containers["make_prg"]
    params:
        pattern="*.fa",
        script=scripts["make_prg"],
        nesting_lvl=config["nesting_level"],
        match_len=config["match_length"],
    log:
        rule_log_dir / "build_local_prgs/{prg_name}.log",
    shell:
        """
        mkdir -p {output.local_prg_dir} 2> {log}
        find {input.msa_dir} -name {params.pattern} | \
        xargs -P {threads} \ 
          sh {params.script} '{{}}' {output.local_prg_dir} {params.nesting_lvl} {params.match_len} {log} 2>> {log}
        """
